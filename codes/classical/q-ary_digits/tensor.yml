# code id, physical, logical are all lower case
code_id: tensor
physical: bits
logical: bits
name: 'Tensor-Product Code'
introduced: '\cite{doi:10.1109/ITW2.2006.323741}'
description: | 
  Given two linear codes \(C_1\) and \(C_2\) over the same alphabet with parity check matrices \(H_1\) and \(H_2\) the tensor code \(C_1 \otimes C_2\) is a 
  linear code whose parity check matrix is given by \(H_1 \otimes H_2 \) which acts on an \( n_1n_2 \)-dimensional vector space, the code space for \(C_1 \otimes C_2\).
  The tensor code is then defined as the linear code in \(\subset F^{n_1 \times n_2}\) satisfying the parity check equation \(H_1 C H^{\top}_2 = 0\). This ensures 
  that $C$ has exactly $r_1r_2$ parity check positions. 

protection: | 
  The distance of the product code is \(d_1 x d_2\), where \(d_1= \text{dist}(C1)\) and \(d_2=\text{dist}(D2)\). They correct any combination of errors of weight 
  up to \(d_1 d_2/2 \) and some combinations of structured errors.

features:
  rate: 'Rate(\(C_1 \otimes C_2\) = Rate(\(C_1\))Rate(\(C_2\))'
  decoders: |
  The simple decoding algorithm (first decode all columns with \(C_1\), then all rows with \(C_2\) corrects up to \((d_1d_2-1)/4 \) errors. More involved algorithms  
  such as Generalized Minimum Distance Decoding \cite{doi: 10.1109/TIT.1966.1053873} or Min-sum algorithm can decode all errors of weight up to (d_1d_2-1)/2. Finally, 
  error location may be coupled with viterbi decoding for every faulty sub-block: \cite{doi: 10.1109/TMAG.2005.861043}. 
  
realizations:
  - 'Code used in magnetic recording by taking the tensor product of a Reed-Solomon code and a parity-check code: \cite{doi: 10.1109/TMAG.2005.861043}'

relations:
  cousins:
    - code_id: concatenated
  
